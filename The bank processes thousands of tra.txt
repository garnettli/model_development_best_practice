The bank processes thousands of transactions daily, yet currently lacks an automated solution to label transaction purposes efficiently. Without such a system, lines of business (LOBs) must assemble teams of analysts who spend close to a month manually tagging transactions with purposes. Even after this labor-intensive effort, there is no centralized repository for publishing or sharing their findings. This absence of a unified platform leads to duplicated efforts among teams addressing similar requests. Consequently, reports are often inconsistent due to multiple sources of truth, exacerbating inefficiencies and hindering operational alignment.

Furthermore, this manual process introduces a significant lag in reporting. Senior management, seeking immediate insights into sudden drops in deposits or unusual transaction patterns, cannot afford the delays caused by manual tagging. They require near real-time understanding of where funds are moving and the underlying reasons for these shifts to make timely decisions.

Our newly developed hybrid model addresses these challenges by combining business rules with machine learning to score transactions at scale and in near real-time. This dual-layer approach provides enhanced granularity and flexibility, as transactions can simultaneously have multiple purposes. For instance, a transaction originating from a Tesla account might pertain to an inventory invoice, and therefore its purposes could be classified as “Tesla,” “invoice,” and “inventory purchases.” By scoring transactions with both business rules and machine learning, the model offers richer insights, enabling LOBs and senior management to make faster, more informed decisions while eliminating inefficiencies and ensuring consistency across teams.

The transaction purpose identification model employs a combination of business rules, keyword searches, and machine learning to enrich transactions with one or multiple purposes.

Business Rules: These rules are primarily account number-driven and are already familiar to the business. They serve as a foundational layer for tagging transactions based on predefined criteria.

Keyword Search: This method includes 50 broader categories, such as "Inventory Purchase," "Invoice," and "Bank-to-Bank Transfer," which were derived through collaboration with LOB. The keyword search examines the transaction description field along with other fields to assign relevant purposes.

Machine Learning: Using natural language processing (NLP) techniques, the machine learning component analyzes transaction description fields and other variables, such as sender/receiver names, to classify transaction purposes. The model outputs the category with the highest probability.

To ensure comprehensive insights, the model applies all three methods—business rules, keyword searches, and machine learning—to the data. Results from each method are stored in separate columns, allowing users to access multiple layers of insights and compare findings when differences arise.

Dependent Variable Definition and Labeling

The data used for the transaction purpose identification model was initially unlabeled, requiring the creation of proxies to label the data.

Wire Rules: The first step involved using predefined wire rules to identify transactions that were already familiar to the LOB. Transactions that were not covered by the business rules were then passed to the next phase.

Keyword Search Rules: To expand the labeled dataset, the team developed keyword search rules. These rules looked for specific keywords associated with a category, primarily within the OBI field, as well as in fields such as beneficiary name and originating name.

The team engaged in a highly iterative process with the LOB to refine these rules. Proposed rules were tested and shared with the LOB for review. Based on their feedback, keywords were added, removed, or modified to improve accuracy. Through this collaborative process, the team finalized approximately 50 keyword rules. A chart detailing all keyword search logic will be attached below to provide further clarity.

These 50 categories became the training data labels for the model. The machine learning component of the model uses this labeled data to identify patterns beyond the fields used for labeling, such as OBI, beneficiary name, and originating name. It also incorporates additional fields like debit/credit account numbers and sender/receiver name information to capture the pattern of the purpose code effectively. This approach ensures that the model generalizes effectively and captures broader transaction patterns, providing reliable dependent variables for training.


The input data is formatted as a matrix, which is the result of applying multiple NLP techniques to clean and transform the text column. The text column was formed by concatenating seven raw input columns into a single long string, with each string separated by a white space. This transformation ensures that the data is in a structured format suitable for model training and fitting.

The dependent variable is derived from the results of keyword searches, which classify transactions into one of 50 predefined categories. These categories serve as the class labels for the model. In production, the model will use these labels to classify incoming transactions into their respective transaction purpose categories.



Collaborated with the Line of Business (LOB) to develop a master taxonomy for storing all known business rules, primarily account number-driven, with some special rules utilizing additional attributes. Designed merging logic to label a subset of the data using these business rules.
Partnered with the LOB to create 48 keyword-based rules for labeling transactions by searching within the OBI, org_name, and bnf_name fields. These keyword results also run on the data labeled by business rules to provide an additional layer of insights.
Implemented a multi-class classification model to predict purpose codes for transactions not covered by rules or keyword searches. The model was trained using proxy labels derived from the OBI keyword results to learn patterns and make predictions based on the highest probability.
Published the final results back to the ILMS system for the Treasury team’s consumption. The output includes three columns: business rules results, OBI keyword search results, and ML prediction results.


The transaction purpose identification model employs a hybrid framework comprising wires rules, OBI keyword searches, and an ML component to maximize coverage and accuracy while minimizing the risk of misclassification. This approach ensures a structured and efficient methodology for handling complex transactional data.

Wires Rules
The wires rules are predefined and well-understood by the Line of Business (LOB), making them the most reliable component of the framework. These rules are primarily account number-driven and introduce minimal uncertainty. To ensure accuracy and prevent misclassification, a rule-based approach was adopted to label this portion of the data.

OBI Keyword Searches
The OBI keyword searches are the result of an iterative collaboration with the LOB, leveraging their domain expertise to define keywords that align with common transaction categories. This component not only provides an additional layer of insights on top of the wires rules but also acts as the training data for the ML model. Since the data is inherently unlabeled, OBI keyword searches serve as a proxy labeling mechanism.

The current set of keywords represents the best possible iteration based on available information. As the model operates in production, user feedback will be collected to refine and improve the OBI keyword rules over time.
The long-term objective is to increase OBI keyword coverage while reducing reliance on the ML model, thereby minimizing uncertainty and improving interpretability.
ML Component
The ML model is designed to handle the most challenging portion of the data, which is characterized by a high proportion of null or unstructured values.

Some null values stem from a known data issue, which will be addressed by the ILMS 2.0 system scheduled for production in Q3 2025. Others result from unstandardized input formats by customers, making fields such as the OBI field occasionally uninformative.
To mitigate these challenges, the ML model incorporates a broader set of features, including account numbers, sender and receiver information, and the transaction description fields (OBI, bnf_name, and org_name). This holistic feature set ensures the model can identify patterns indicative of transaction purpose codes.
Additionally, the LOB manually reviewed the ML model’s predictions and restricted its use to top-performing categories, significantly reducing the risk of misclassification within this complex dataset.
This hybrid approach ensures that each component contributes to the overall robustness of the framework, effectively balancing the strengths of rule-based methods and machine learning to achieve a comprehensive and reliable solution.



import random
import re
from pyspark.sql.functions import udf, lit
from pyspark.sql.types import StringType

# QWERTY keyboard adjacency mapping
keyboard_adjacent = {
    'q': 'wa', 'w': 'qse', 'e': 'wsdr', 'r': 'etdf', 't': 'ryfg', 'y': 'tugh', 'u': 'yihj',
    'i': 'uojk', 'o': 'ipkl', 'p': 'ol',
    'a': 'qwsz', 's': 'awedxz', 'd': 'serfcx', 'f': 'drtgvc', 'g': 'ftyhbv', 'h': 'gyujnb',
    'j': 'huikm', 'k': 'jiolm', 'l': 'kop',
    'z': 'asx', 'x': 'zsdc', 'c': 'xdfv', 'v': 'cfgb', 'b': 'vghn', 'n': 'bhjm', 'm': 'njk',
}

# Words to exclude from typo generation
EXCLUDE_WORDS = {"miss"}  # Add more words if needed

def is_number(word):
    """Check if a word consists only of digits."""
    return bool(re.fullmatch(r"\d+", word))  # Matches only numeric words

def introduce_typos(text, typo_ratio):
    """Introduce typos while excluding specific words and numbers."""
    if not text or typo_ratio <= 0:
        return text  # Return unchanged if empty or no typo needed

    words = text.split()  # Tokenize by whitespace
    for i in range(len(words)):
        if words[i] in EXCLUDE_WORDS or is_number(words[i]):
            continue  # Skip excluded words and numbers

        word = list(words[i])  # Convert to list for mutability
        num_typos = int(len(word) * typo_ratio)  # Compute number of typos

        for _ in range(num_typos):
            idx = random.randint(0, len(word) - 1)  # Pick a random character index
            char = word[idx]

            typo_type = random.choice(["swap", "replace", "delete", "insert"])

            if typo_type == "swap" and idx < len(word) - 1:
                word[idx], word[idx + 1] = word[idx + 1], word[idx]
            elif typo_type == "replace" and char.lower() in keyboard_adjacent:
                word[idx] = random.choice(keyboard_adjacent[char.lower()])
            elif typo_type == "delete" and len(word) > 1:
                word.pop(idx)
            elif typo_type == "insert" and char.lower() in keyboard_adjacent:
                word.insert(idx, random.choice(keyboard_adjacent[char.lower()]))

        words[i] = "".join(word)  # Convert back to string

    return " ".join(words)  # Reconstruct sentence

# Convert the function into a PySpark UDF
typo_udf = udf(lambda text, ratio: introduce_typos(text, float(ratio)), StringType())

# Apply different typo ratios
df_test = df_test.withColumn("typo_10", typo_udf(df_test["stemmed_words"], lit(0.1)))
df_test = df_test.withColumn("typo_20", typo_udf(df_test["stemmed_words"], lit(0.2)))
df_test = df_test.withColumn("typo_30", typo_udf(df_test["stemmed_words"], lit(0.3)))

# Show results
df_test.select("stemmed_words", "typo_10", "typo_20", "typo_30").show(truncate=False)




# Sensitivity Analysis and Testing

## 1. Objective

The objective of this sensitivity analysis is to evaluate the robustness of the NLP classification model against human-like typographical errors in transaction descriptions. Given that real-world data may contain typos due to manual entry, it is essential to assess how well the model performs when exposed to such perturbations. This analysis ensures that the model remains reliable in practical scenarios where minor textual variations occur.

## 2. Overview of Methodology

To conduct this sensitivity analysis, we introduced controlled levels of typographical errors into the input text column of the dataset. Specifically, random character modifications such as swaps, deletions, insertions, and replacements were applied at predefined typo rates: **5%, 10%, and 13%**. These rates were chosen based on:

- Studies indicating that typical human typists exhibit a typo rate between **1% and 5%**.
- The nature of our dataset, which consists of wire transaction descriptions **manually entered by bank personnel**, rather than casual or mobile device inputs.
- The structured nature of long transaction descriptions, which often follow a **template format**, inherently reduces the likelihood of frequent typographical errors, whereas shorter descriptions are manually created and may be more prone to typos.

Given these factors, **5% was selected as a conservative baseline**, **10% as a moderate stress test**, and **13% as a high-stress scenario** to observe performance degradation beyond normal human error levels.

### Keyboard Adjacency Mapping and Typo Injection

A **keyboard adjacency mapping** was created to simulate realistic typographical errors based on common QWERTY keyboard mistakes. This mapping associates each letter with its adjacent keys, simulating likely mispresses. Below is an example of the adjacency mapping used:

| Letter | Adjacent Keys |
| ------ | ------------- |
| q      | wa            |
| w      | qse           |
| e      | wsdr          |
| r      | etdf          |
| t      | ryfg          |
| y      | tugh          |
| u      | yihj          |
| i      | uojk          |
| o      | ipkl          |
| p      | ol            |
| a      | qwsz          |
| s      | awedxz        |
| d      | serfcx        |
| f      | drtgvc        |
| g      | ftyhbv        |
| h      | gyujnb        |
| j      | huikm         |
| k      | jiolm         |
| l      | kop           |
| z      | asx           |
| x      | zsdc          |
| c      | xdfv          |
| v      | cfgb          |
| b      | vghn          |
| n      | bhjm          |
| m      | njk           |

We applied four types of typo modifications:

1. **Swap:** Two adjacent characters are swapped (e.g., `fraud → fardu`).
2. **Replace:** A character is replaced with a randomly selected adjacent key (e.g., `wire → wjre`).
3. **Delete:** A character is randomly deleted (e.g., `money → moey`).
4. **Insert:** A random adjacent character is inserted next to an existing character (e.g., `bank → baank`).

### Application of Typo Rate

The typo rate was applied by selecting a percentage of characters within each transaction description for modification. The process involved:

- Calculating the number of characters to be altered based on the total length of the string and the specified typo rate.
- Rounding up to ensure at least one modification is applied even for short words.
- Randomly choosing positions within the string to apply modifications.
- Ensuring that numeric values and specific words such as 'miss' remained unaltered.
- Applying a mix of swap, replace, delete, and insert operations proportionally across the selected characters.

For example, given the input `transaction complete`, applying a **10% typo rate** (rounding up to affect 2 characters) could yield:

- Swap (affects 2 characters at once): `trnasaction complete` (swapping 'r' and 'n')
- Replace (affects 2 characters through separate replacements): `tarnsactuon complete` (replacing 'r' with 'a' and 'i' with 'u')
- Delete (affects 2 characters by removing them): `trasactin complete` (removing 'n' and 'o')
- Insert (affects 2 characters by adding new ones): `trannasaction complete` (inserting an extra 'n' and 'a')

## 3. Sensitivity Analysis and Adversarial Testing

The experiment was conducted by evaluating the model’s accuracy across different levels of induced typographical noise. The results were as follows:

| Typo Rate              | Model Accuracy      |
| ---------------------- | ------------------- |
| **0% (Original Data)** | **79.00%**          |
| **5% Typos**           | **79.34%** (+0.34%) |
| **10% Typos**          | **78.21%** (-0.79%) |
| **13% Typos**          | **71.99%** (-7.01%) |

### Key Observations:

- **5% Typo Rate:** Minimal impact on model accuracy (+0.34%), suggesting the model is robust to minor typographical variations.
- **10% Typo Rate:** Slight decrease in accuracy (-0.79%), indicating some sensitivity to more frequent typos.
- **13% Typo Rate:** Significant accuracy drop (-7.01%), highlighting that extensive noise can degrade model performance.

These findings suggest that while the model remains stable under realistic typo scenarios (≤10%), **extensive typographical errors can meaningfully impact classification accuracy**.

## 4. Conclusion

The sensitivity analysis demonstrates that the model is **robust to minor typographical errors**, especially at a realistic 5% typo rate, where no meaningful accuracy loss was observed. However, as the typo rate increases beyond 10%, performance degradation becomes noticeable, with a sharp decline at 13%.

These results validate that the model is well-suited for real-world applications where transaction descriptions are manually entered by bank personnel, as long as typo rates remain within reasonable human error thresholds. Future improvements could explore additional robustness techniques, such as **character-level embeddings** or **spell correction mechanisms**, to further mitigate classification errors under high-stress conditions.



Studies indicate that typical human typists exhibit error rates between 1% and 5%. For instance:

A study published in Control Engineering notes that even skilled typists have error rates around 1 error per 100 words, equating to a 1% error rate. 
CONTROLENG.COM

Research on manual data entry methods highlights that human error rates are approximately 1-5%, which can impact data accuracy and quality assurance. 
RESEARCHGATE.NET

These findings suggest that typical human typists have error rates within the 1% to 5% range.


import random
import re
from pyspark.sql.functions import udf, lit
from pyspark.sql.types import StringType

# QWERTY keyboard adjacency mapping
keyboard_adjacent = {
    'q': 'wa', 'w': 'qse', 'e': 'wsdr', 'r': 'etdf', 't': 'ryfg', 'y': 'tugh', 'u': 'yihj',
    'i': 'uojk', 'o': 'ipkl', 'p': 'ol',
    'a': 'qwsz', 's': 'awedxz', 'd': 'serfcx', 'f': 'drtgvc', 'g': 'ftyhbv', 'h': 'gyujnb',
    'j': 'huikm', 'k': 'jiolm', 'l': 'kop',
    'z': 'asx', 'x': 'zsdc', 'c': 'xdfv', 'v': 'cfgb', 'b': 'vghn', 'n': 'bhjm', 'm': 'njk',
}

# Words to exclude from typo generation
EXCLUDE_WORDS = {"miss"}  # Add more words if needed

def is_number(word):
    """Check if a word consists only of digits."""
    return bool(re.fullmatch(r"\d+", word))  # Matches only numeric words

def introduce_typos(text, typo_ratio):
    """Introduce typos while excluding specific words and numbers, ensuring at least 1 typo in the entire document."""
    if not text or typo_ratio <= 0:
        return text  # Return unchanged if empty or no typo needed

    words = text.split()  # Tokenize by whitespace
    valid_word_indices = [i for i, word in enumerate(words) if word not in EXCLUDE_WORDS and not is_number(word)]
    
    if not valid_word_indices:
        return text  # If no valid words to modify, return original text

    total_chars = sum(len(words[i]) for i in valid_word_indices)  # Count total modifiable characters
    total_typos = max(1, int(total_chars * typo_ratio))  # Ensure at least 1 typo for the entire document

    for _ in range(total_typos):
        word_idx = random.choice(valid_word_indices)  # Choose a random valid word index
        word = list(words[word_idx])  # Convert to list for mutability
        
        if len(word) == 0:
            continue  # Skip empty words
        
        char_idx = random.randint(0, len(word) - 1)
        char = word[char_idx]

        typo_type = random.choice(["swap", "replace", "delete", "insert"])

        if typo_type == "swap" and char_idx < len(word) - 1:
            word[char_idx], word[char_idx + 1] = word[char_idx + 1], word[char_idx]
        elif typo_type == "replace" and char.lower() in keyboard_adjacent:
            word[char_idx] = random.choice(keyboard_adjacent[char.lower()])
        elif typo_type == "delete" and len(word) > 1:
            word.pop(char_idx)
        elif typo_type == "insert" and char.lower() in keyboard_adjacent:
            word.insert(char_idx, random.choice(keyboard_adjacent[char.lower()]))

        words[word_idx] = "".join(word)  # Update the modified word in the list

    return " ".join(words)  # Reconstruct the document text

# Convert the function into a PySpark UDF
typo_udf = udf(lambda text, ratio: introduce_typos(text, float(ratio)), StringType())

# Apply different typo ratios in a PySpark DataFrame
df_test = df_test.withColumn("typo_05", typo_udf(df_test["stemmed_words"], lit(0.05)))
df_test = df_test.withColumn("typo_10", typo_udf(df_test["stemmed_words"], lit(0.1)))
df_test = df_test.withColumn("typo_20", typo_udf(df_test["stemmed_words"], lit(0.2)))

# Show results
df_test.select("stemmed_words", "typo_05", "typo_10", "typo_20").show(truncate=False)

