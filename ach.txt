from pyspark.sql.functions import expr, lit
from pyspark.sql import DataFrame

# Columns used for matching
matching_columns = [
    "dr_acct_num", "cr_acct_num",
    "dr_fin_inst", "cr_fin_inst",
    "dr_lob", "cr_lob",
    "dr_acct_nm", "cr_acct_nm",
    "tentr", "ncorg", "tdisc"
]

# Load rules to driver (1300 rules is safe)
rules = like_rules_df.collect()

matched_df = None

# Loop over each rule
for rule in rules:
    conditions = []

    for colname in matching_columns:
        val = rule[colname]
        
        if val is None:
            continue  # Skip nulls â€” match anything
        
        elif '%' in val:
            # Respect wildcard position
            regex = val.replace('%', '.*')
            if not val.startswith('%'):
                regex = '^' + regex
            if not val.endswith('%'):
                regex = regex + '$'
            conditions.append(f"{colname} RLIKE '{regex}'")
        
        else:
            # Exact match
            conditions.append(f"{colname} = '{val}'")
    
    # Skip rule if no matching conditions (all columns null)
    if not conditions:
        continue

    condition_expr = " AND ".join(conditions)

    matched = transactions_df.filter(expr(condition_expr)) \
        .withColumn("rule_id", lit(rule['rule_id'])) \
        .withColumn("purpose_lvl1", lit(rule['purpose_lvl1'])) \
        .withColumn("purpose_lvl2", lit(rule['purpose_lvl2'])) \
        .withColumn("purpose_lvl3", lit(rule['purpose_lvl3'])) \
        .withColumn("purpose_lvl4", lit(rule['purpose_lvl4']))

    matched_df = matched if matched_df is None else matched_df.unionByName(matched)


from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

window = Window.partitionBy("transaction_id").orderBy("rule_id")  # or use custom priority
final_matched = matched_df.withColumn("rn", row_number().over(window)).filter("rn = 1")


result = transactions_df.join(
    final_matched.select("transaction_id", "purpose_lvl1", "purpose_lvl2", "purpose_lvl3", "purpose_lvl4"),
    on="transaction_id",
    how="left"
)




#cross join and filter

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, lit, udf, monotonically_increasing_id, row_number,
    when
)
from pyspark.sql.window import Window
from pyspark.sql.types import StringType

# 1. Initialize Spark
spark = SparkSession.builder.getOrCreate()

# 2. Regex-safe conversion for wildcard rules
def to_safe_regex(val):
    if val is None:
        return None
    val = val.replace("'", "''")  # escape single quote for Spark SQL
    val = val.replace("%", ".*")
    if not val.startswith('%'):
        val = '^' + val
    if not val.endswith('%'):
        val = val + '$'
    return val

regex_udf = udf(to_safe_regex, StringType())

# 3. Columns to match on
matching_columns = [
    "dr_acct_num", "cr_acct_num",
    "dr_fin_inst", "cr_fin_inst",
    "dr_lob", "cr_lob",
    "dr_acct_nm", "cr_acct_nm",
    "tentr", "ncorg", "tdisc"
]

# 4. Preprocess rules_df: create is_like + regex_safe columns
def preprocess_rules(rules_df, match_cols):
    for colname in match_cols:
        rules_df = rules_df.withColumn(
            f"{colname}_is_like",
            when(col(colname).contains('%'), lit(True)).otherwise(lit(False))
        ).withColumn(
            f"{colname}_regex_safe",
            when(col(colname).contains('%'), regex_udf(col(colname))).otherwise(None)
        )
    return rules_df

# Example usage:
# rules_df = preprocess_rules(rules_df, matching_columns)

# 5. Add txn_id to transactions_df
transactions_df = transactions_df.withColumn("txn_id", monotonically_increasing_id())

# 6. Broadcast rules and cross join
rules_df = preprocess_rules(rules_df, matching_columns)
rules_broadcast = rules_df.select(
    ["rule_id", "purpose_lvl1", "purpose_lvl2", "purpose_lvl3", "purpose_lvl4"] +
    [f"{col}_is_like" for col in matching_columns] +
    [f"{col}_regex_safe" for col in matching_columns] +
    matching_columns
)

joined_df = transactions_df.crossJoin(rules_broadcast)

# 7. Build filtering condition
condition = lit(True)
for colname in matching_columns:
    is_like_col = f"{colname}_is_like"
    regex_col = f"{colname}_regex_safe"
    rule_val_col = col(colname)
    txn_val_col = col(f"transactions_df.{colname}") if f"transactions_df.{colname}" in joined_df.columns else col(colname)

    condition = condition & (
        rule_val_col.isNull() |
        (col(is_like_col) & txn_val_col.rlike(col(regex_col))) |
        (~col(is_like_col) & (txn_val_col == rule_val_col))
    )

filtered_df = joined_df.filter(condition)

# 8. Resolve multiple rule matches by keeping top rule per txn
w = Window.partitionBy("txn_id").orderBy("rule_id")
final_matched = filtered_df.withColumn("rn", row_number().over(w)).filter("rn = 1")

# 9. Join purpose codes back to transactions_df
transactions_with_purpose = transactions_df.join(
    final_matched.select(
        "txn_id", "purpose_lvl1", "purpose_lvl2", "purpose_lvl3", "purpose_lvl4"
    ),
    on="txn_id",
    how="left"
)

# transactions_with_purpose now contains enriched results
